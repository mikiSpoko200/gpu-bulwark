\chapter{OpenGL and GLSL}

\section{Introduction}

OpenGL is an specification of an API for hardware accelerated computer graphics owned and maintained by the Khronos Group Inc.

Since it's inception  and up until 2016 with release of Vulkan it has been the only widely supported cross platform graphics API.

\section{History}

IRIS GL, a proprietary graphics API, which later became OpenGL was initially developed by Silicon Graphics (SGI) 
during 1980's. SGI open sourced subset of their API as OpenGL due to mounting market pressure from SGI competitors (Sun Microsystems, Hewlett--Packard, IBM) 
who started providing their own APIs for hardware accelerated 3D graphics based on an existing open standard called PHIGS.
In 1992 OpenGL Architectural Review Board (OpenGL ARB) was established and it was tasked with development and maintenance of the OpenGL specification. 
This task was passed on to Khronos Group in 2006 where it remained ever since.

\section{OpenGL objects}

OpenGL exposes an abstraction over GPU's resources called objects.
These roughly correlate with object oriented design as they aggregate data for appropriate subset of operations albeit with certain unique caveats.
In all but the latest opengl versions, to use given object it first must have been bound to a binding point in global in current OpenGL context.
In OpenGl 4.6 the \texttt{ARB\_direct\_state\_access} extension was made part of core specification which introduces duplicates of all object manipulating functions to 
accept as one of parameters the name of object to operate on.

Objects contain internal state which can be queried using introspection. 
Objects are identified by a \textit{name} which is an unsigned 32 bit integer. 
There exists common object model which describes how most types of objects are managed.

\noindent Most types of objects can be created with a call to
\begin{center}
    \texttt{void} \textbf{Gen*s}(\texttt{sizei} \textit{n}, \texttt{uint} *\textit{objects})
\end{center}
\noindent which will allocate the object's \textit{name}. A subsequent call to
\begin{center}
    \texttt{void} \textbf{Bind*}(\texttt{uint} \textit{name}, \ldots)
\end{center}
\noindent will bind the given object to the context. If the object has never been bound before, this will also allocate its internal state. Alternatively, one can use
\begin{center}
    \texttt{void} \textbf{Create*s}(\texttt{sizei} \textit{n}, \texttt{uint} *\textit{objects})
\end{center}
\noindent which will allocate both the object's \textit{name} and its state, but it will not set any context bindings. There exists a separate namespace for each object type.

Objects can be deleted with \texttt{void} \textbf{Delete*s}(\texttt{sizei} \textit{n}, \texttt{uint} *\textit{objects}), bound with aforementioned \texttt{void} \textbf{Bind*}(\texttt{uint} \textit{name}, ...) which usually accepts additional parameter that specifies binding point.
The most notable outliers that do not conform to the rules above are program objects and shader objects.

OpenGL specification defines set of publicly available object parameters which can be queries using introspection with \textbf{GetInteger*} family of functions. One notable usage is determining compilation and linking status for shaders and programs. 
\subsection{Buffer object}

Buffer objects provide means to allocate data stores in GPU memory. They can contain data of different format and purpose depending on buffer's target. Primary usage for buffers is to provide geometric information which includes vertex attribute values and indices for indexed rendering.

\subsection{Vertex Array object}

Modern OpenGL is generic over vertex format and only poses limitation on the number of such 
attributes and limits their values to s scalar or vector types.
Each attribute is assigned a zero--index. Vertex Array object (VAO) assigns each active 
attribute information on how and where from to source vertex data, as well as, what is the
data type of provided attribute in glsl.

This can be viewed as two aspects: (1) memory layout and access frequency, and (2) data interpretation / conversion.

\section{Graphics pipeline}
The modern OpenGL pipeline is a sequence of both programmable and fixed function stages that process geometric data to form discrete color values --- pixels --- that end up stored in a framebuffer. 

\subsection{Vertex Specification}

Before rendering can begin, geometric information needs to uploaded to GPU memory along with its description as generic vertex attributes.

Generic Vertex is an abstract composition of values (attributes) that is supposed to represent a vertex of the triangular mesh of an object.
Generic stands from the fact that data associated with vertices has no intrinsic meaning.
Semantics of data are decided by client provided vertex shader.

OpenGL sources data for each vertex attribute form a buffer. Each attribute is assigned a unique numeric index.
Association between attribute with given index and a buffer, from which that attribute should be sourced, is established by Vertex Array Object (abbr. VAO).

Once all vertex attributes have their data sources assigned and properly configured, vertex specification can be considered finished and one could precede with further pipeline configuration.
In this instance vertices would be interpreted sequentially as appropriate geometric primitives. This forces vertex data to be specified redundantly for 
lines and especially raw triangles, since each triangle shares an edge with each neighboring triangle.

To better conserve memory one can use indexed rendering. This requires additional buffer filled with indices into main vertex buffer instead of inlined vertex data.
In case of basic triangle rendering (without using compressed representations like triangle fan or triangle / line strip) will still cause repetition but now only few byte wide indices instead of whole attributes which are substantially larger.

\subsection{Vertex Shader}

Vertex shader is the first programmable stage of OpenGL Pipeline and is one of two required shaders to execute a draw call, the other being the fragment shader.

Most commonly vertex shader performs 3 transformations. From initial model space, world space, view space to final clip space which we will now discuss briefly.

\textbf{model space} --- when a 3D model is created in 3D modelling software its vertex positions are specified to some local coordinate system (commonly center of an object).
These positions would commonly be loaded into GPU memory. Such objects can be easily placed in broader scene by providing a so called world transform performs transformation from model's local coordinate system
to scene's coordinate system.

\textbf{world space} --- world space refers to coordinate system of a scene that uses multiple models by translating, scaling or rotating them.

\textbf{view space} --- its common for 3D rendering applications to provide means of interacting with the scene. Whether its a 3D computer game, CAD program or medical data visualization we would like to be able to 
control how scene is displayed by moving a virtual camera. This can be expressed as yet another transformation of the coordinate system --- we would like to transform coordinate system to align with the position of our camera.
This transformation is commonly called view transform.

\textbf{clip space} --- having accounted for model position in a scene and user interactivity all that remains is to provide vertex data in form that subsequent fixed function pipeline stage --- the rasterizer --- expects.
Namely once vertex shader is finished fixed function processing will clip all geometry then perform perspective divide to obtain vertices in normalized device coordinates (NDC).
Output of vertex shader is a 4 component vector which corresponds to a 3D position in homogenous coordinate system used in computer graphics due to its ability to represent non linear transformations using matrices.
The output position of vertex is divided by the forth component in order to introduce perspective 

\subsection{Tessellation}

Tesselation stages were added as graphics hardware compute capability grew. With raw compute throughput outperforming bus throughput GPUs were equipped with hardware
tesselation unit which can subdivide a larger triangle into batch of smaller ones. This allows for efficient generation of geometric detail on chip alleviating the 
issue of limited PCI throughput.
To drive the tesselation stage two new shaders were introduced:
\begin{itemize}
    \item \textbf{Tesselation control shader} which configures how hardware tesselator should subdivide a triangle.
    \item \textbf{Tesselation evaluation shader} which performs transformations on vertices generated by the tesselator.
\end{itemize}

\subsection{Geometry shading}

Geometry shader was introduced prior to tesselation stage. They operate on assembled geometric primitives and may even access primitives neighbors.
Given primitive input geometry shaders output one or more geometric primitives. Output primitives all must be of the same type which can be different then the input primitive type.
may be of different type different from original input, for example given single points shader can emit 2 triangles.

\subsection{Fixed function vertex post--processing}

Once all programmable vertex progressing has concluded, a series of fixed--function operations are applied to the vertices of the resulting primitives before rasterization.
These operations include transform feedback, which captures processed vertex data, 
primitive queries to gather information about the primitives being processed 
and flat shading which applies a uniform attribute value to a whole primitive.

Primitives then get clipped against clip volume and client--defined half--spaces.
The clip coordinates undergo perspective division, followed by viewport mapping to adjust for screen coordinates and depth range scaling.

\subsection{Rasterization}

If neither tesselation stage nor geometry stage was used in vertex processing, primitive assembly takes place (presence of any of the aforementioned stages would necessitate early primitive assembly). 
OpenGL converts geometric primitives used in currently processed draw call into base primitives which are points, lines and triangles.
Mathematical representation of primitives is used during rasterization to determine if given fragment falls inside of primitive being rasterized.

Process of rasterization requires determining if given pixel position falls inside of rendered primitive. This process needs to account for point and line thickness.
Polygon rasterization is obviously the most complex of the three. Prior to insidness test face culling is performed. 
This optimization culls a polygon based on the sign of surface normal computed based on edge ordering as specified in vertex array.
This helps reduce overdraw which can be one of two main bottlenecks in modern rendering system, the latter being insufficient memory bandwidth.

Once pixel location was deemed inside a primitive a fragment is generated. 
A Fragment is a collection of data corresponding to specific pixel location.
Most commonly its perspective corrected barycentric interpolation of vertex data across the primitive's surface.
Tough interpolation can be disabled from within vertex shader using \texttt{flat} qualifier on output variable declaration,
as well as perspective correction with \texttt{noperspective} qualifier.

Once fragments are computed early per--fragment tests take place.

\begin{itemize}
    \item \textbf{Ownership test} --- determines if pixel at location (x, y) falls into the portion of the screen that active OpenGL context owns.
    \item \textbf{Scissor test} --- checks if pixel at location (x, y) is contained within client provided list of axis aligned rectangles
    \item \textbf{Early Fragment tests} --- stencil test, depth test and occlusion query which are normally performed after fragment processing can optionally be performed early. We discuss them in subsection on fragment post processing.
\end{itemize}

If all tests passed fragment is submitted for programable fragment processing.

\subsection{Fragment processing}

Programable fragment processing is performed by client provided fragment shader. The most essential task that fragment shader should perform is assign pixel a color.
For that purpose data interpolated from rasterization is used. 
Most commonly fragment shaders perform texture mapping, lighting calculations, parallax mapping to emulate geometric detail and screen space effects like ambient occlusion, 
use signed distance functions and implicit surface equations to render otherwise complex scenes all by itself or create volumetric effects like clouds or visualize CT scan results.

\subsection{Fragment post processing}

After all fragments are processed fixed function processing takes over for the final time.
Aforementioned ownership, scissor tests take place followed by stencil and depth test.
Each of these test require additional buffer to be allocated, called appropriately stencil buffer and depth buffer.

Once enabled, the stencil test works by comparing a reference value against the value stored in the stencil buffer according to a specified comparison function. 
Depending on the result of this comparison, the pixel may be drawn, modified, or discarded. 
Functions \texttt{glStencilFunc} and \texttt{glStencilOp} customize when fragment should pass the test and whether to update current values in stencil buffer.

The depth test is much more commonly used. 
When fragments are rendered and depth test is enabled, based on their \texttt{xy} coordinates, their \texttt{z} coordinate is compared against current value in the depth buffer.
Typically, if tested fragment's depth value is greater then the one stored in the buffer we can reject it since a fragment we processed earlier occludes current fragment.
However, we can customize test outcome based on comparison result using \texttt{glDepthFunc}.

Occlusion queries count how many fragments passed depth test. This information can be read and used to implement amongst others: post processing effects, occlusion culling, Level Of Detail.

Finally alpha blending takes place. This process blends together values of current fragment with what's already stored in the framebuffer.
Exactly what operation is performed can be configured to a limited degree similarly to functions determining outcome of depth and stencil test.

\section{GLSL}

GLSL, which stands for OpenGL Shading Language, is a high level shading language with c like syntax developed by OpenGL Architecture Review Board to power programable processing stages in OpenGL pipeline. 
GLSL code is still relevant as it can be compiled into SPIR--V and used with Vulkan API.

\subsection*{Shaders}

Independent compilation units written in this language are called shaders. A program is a set of
shaders that are compiled and linked together, completely creating one or more of the

In OpenGL 4.6 and GLSL 4.60 there exist 6 types of shaders: vertex, tesselation control and evaluation, geometry, fragment and compute.
All shaders except compute shader control appropriate parts of OpenGL pipeline as described in subsections above. 

Compute shaders operate completely outside of graphics pipeline. They can access same resources as fragment or vertex shader like textures, buffers, images and atomic counters
but they are not expected to produce data with predetermined form or semantics. They offer general purpose compute capability on the GPU. 
They function similarly to other existing general purpose GPU compute APIs like CUDA or OpenCL.

\subsection{Variables}

The main purpose of shaders if to transform received data to some other form. The data that the shader expects is defined using global variables with appropriate qualifiers.
During Program linking OpenGL matches outputs from previous stage with inputs of the next stage. 
In case of vertex shader \texttt{in} variables should match with vertex attribute definitions specified in vertex array object.
Tough in case of mismatch if attribute is disabled constant value can be provided, however thats rarely desired behavior.
Similarly, \texttt{out} variables from fragment shader should match with framebuffer configuration.
This process can be quite error prone and can lead to undefined behavior which can be difficult to diagnose, and may have different consequences depending on actual hardware, os or driver versions.

Under no circumstances erroneous pipeline configuration should be allowed. Program containing such malformed configuration should be rejected by static analysis, and that was one of most important aspects of this study.
To achieve that we attempted to express both GLSL variable declarations along with full OpenGL pipeline in Rust's type system in such a way to force type errors for invalid pipeline configurations.
We determined that keeping track of these three variable qualifiers is essential to achieve that.

\subsection{Variable types}

Expressing GLSL variable type in Rust types was the obvious first step. GLSL defines a set of built--in types along with ability to create aggregate data types with C--like array and struct definitions.

For this work we focused on builtin types and arrays and omitted structures due to Rust's inability to encode layout guarantees for arbitrary types.
Rust's built--in numeric types and arrays have however have well defined memory layout and create a close set of possible types which allowed us to enumerate them express their memory layout in type system using traits.

GLSL's built--in types are divided into two groups: transparent and opaque types.
Transparent types represent numeric data (plain old data) whereas opaque types represent handles to different resources like texture image samplers.

In case of transparent types there are 5 base numeric types: \texttt{float}, \texttt{double}, \texttt{int}, \texttt{uint} and \texttt{bool}.
Floating point types \texttt{float} and \texttt{double} are accordingly IEEE-754 single and double prevision numbers, integers are two's compliment 32bit values and bool undefined representation but it can take only two values \texttt{true} or \texttt{false}.

All base numeric types can be aggregated into 2, 3 or 4 component vector types. Each vector type is named \texttt{\textbf{T}vec\textbf{N}}
where \textbf{T} depends on inner base type: \texttt{b} for \texttt{bool}, \texttt{d} for \texttt{double}, \texttt{i} for \texttt{int}, \texttt{u} for \texttt{uint} and for \texttt{float} nothing is prepended to \texttt{vec\textbf{N}}.
\textbf{N} is the number of components vector should contain.

Finally, there are matrix types of form \texttt{\textbf{T}mat\textbf{N}}. Matrices can contain only \texttt{float}s (\texttt{mat\textbf{N}}) or \texttt{double}s (\texttt{dmat\textbf{N}}).
The \textbf{N} depends on matrix dimensions it can be a single number 2, 3 or 4 for square matrices or can be arbitrarily combined pair of these numbers of form \texttt{\textbf{N}x\textbf{M}},
i.e. \texttt{mat2x4}, \texttt{mat4x2} or \texttt{dmat3x3}.

Data types used by GLSL have quite large memory footprint. Thats why OpenGL provides conversion mechanisms for data stored in buffers. Buffer data can be low bitwidth integer or float which will be normalized on access or even completely 
new OpenGL defined packed formats like \texttt{UNSIGNED\_INT\_2\_10\_10\_10\_REV} --- a 32bit value which will be expanded to 4 \texttt{float}s. 

This indirect mapping of OpenGL data to GLSL types is also essential to be statically verified just like shader input / output matching.

\subsection{Variable storage qualifiers}

The origin of data for a variable is encoded by a storage qualifier. We have already discussed that data within a shader can originate from previous stage / vertex buffers and that it can be saved as input to subsequent stage or framebuffer.
These sources correspond to \texttt{in} and \texttt{out} qualifiers. 
Shaders can also declare uniform variables which are data associated with program itself. Value of these variables remains the same across the
entire primitive being processed. All uniform variables are read--only and are initialized externally either at link time or through the API.

For bidirectional communication between shaders and API there exists \texttt{buffer} qualifier. Variables with such qualification are stored in buffer objects and can be both written to and read from by shaders and API.

Remaining qualifiers are ignored as they are irrelevant for the scope of this study.

\subsection{Variable layout qualifiers}

Statically asserting that the data passed though the pipeline is correct was the main goal of this study.
Ability to statically assert that data flow though the pipeline is configured correctly depends on our ability to match neighboring stage inputs and outputs.
OpenGL specification calls this process \textit{shader interface matching}.

Historically interface matching would match variables based on their names and types.
Wiliest still valid, in newer OpenGL versions there exist a better way to match shader stages. 
One can use \texttt{location} layout qualifier to assign a variable a integer index.
This integer will be checked for uniqueness among all other variables that it shares storage qualifier with; program linking will fail in case of overlap.
When using locations based interface matching each \texttt{out} variable in considered to match an input variable in the subsequent shader if
the two variables are declared with the same location and component layout qualifiers and match in type and qualification.

Together \texttt{location}, storage and type qualifiers uniquely define a variable for the purposes of interface matching.
